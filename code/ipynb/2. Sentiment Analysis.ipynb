{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0AI91E2TiB6b"
   },
   "source": [
    "# 2. Perform sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N_vLElmSiB6e"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from tweepy import Stream\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import StreamListener\n",
    "import re\n",
    "import json\n",
    "#from HTMLParser import HTMLParser\n",
    "from html.parser import HTMLParser\n",
    "from textblob import TextBlob\n",
    "#from keys_access_tokens import consumer_key, consumer_secret, access_token, access_secret\n",
    "# Save your own keys and access tokens in \"keys_access_tokens.py\" acquired from Twitter Apps (https://apps.twitter.com/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zjTHgd44iB6l"
   },
   "source": [
    "## 1. Loading the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T9Mq1YVxiB6n"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from nltk.classify import ClassifierI\n",
    "from statistics import mode\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "goUZX1OEiB6s"
   },
   "outputs": [],
   "source": [
    "class VoteClassifier(ClassifierI):\n",
    "    def __init__(self, *classifiers):\n",
    "        self._classifers = classifiers\n",
    "        \n",
    "    def classify(self, features):\n",
    "        votes = []\n",
    "        for c in self._classifers:\n",
    "            v = c.classify(features)\n",
    "            votes.append(v)\n",
    "        return mode(votes)\n",
    "        \n",
    "    def confidence(self, features):\n",
    "        votes = []\n",
    "        for c in self._classifers:\n",
    "            v = c.classify(features)\n",
    "            votes.append(v)\n",
    "        \n",
    "        choice_votes = votes.count(mode(votes))\n",
    "        confidence_factor = choice_votes / len(votes)\n",
    "\n",
    "        return confidence_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OGGPmYuxiB6x"
   },
   "outputs": [],
   "source": [
    "def find_features(document):\n",
    "    words = word_tokenize(document)\n",
    "    features = {}\n",
    "    for w in word_features:\n",
    "        features[w] = (w in words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qzqyx-WYiB62"
   },
   "outputs": [],
   "source": [
    "def sentiment(text):\n",
    "    features = find_features(text)\n",
    "    return voted_classifier.classify(features), voted_classifier.confidence(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 229
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2038,
     "status": "error",
     "timestamp": 1556216047547,
     "user": {
      "displayName": "Souradeepta Biswas",
      "photoUrl": "https://lh5.googleusercontent.com/-8iBJB6yxUuY/AAAAAAAAAAI/AAAAAAAAFIM/TkUD5spEYUA/s64/photo.jpg",
      "userId": "05312634483470100056"
     },
     "user_tz": 240
    },
    "id": "59VkDyFuiB67",
    "outputId": "895e920c-2282-42be-ca5a-ff31cd9ca47b"
   },
   "outputs": [],
   "source": [
    "documents_file = open(\"pickles/documents.pickle\", \"rb\") \n",
    "documents = pickle.load(documents_file)\n",
    "documents_file.close()\n",
    "\n",
    "word_features_file = open(\"pickles/word_features.pickle\", \"rb\")\n",
    "word_features = pickle.load(word_features_file)\n",
    "word_features_file.close()\n",
    "\n",
    "\n",
    "file = open(\"pickles/Naive_Bayes.pickle\", \"rb\") \n",
    "NB_classifier = pickle.load(file)\n",
    "file.close()\n",
    "\n",
    "file = open(\"pickles/Multinomial_NB.pickle\", \"rb\") \n",
    "MNB_classifier = pickle.load(file)\n",
    "file.close()\n",
    "\n",
    "file = open(\"pickles/Bernoulli_NB.pickle\", \"rb\") \n",
    "BernoulliNB_classifier = pickle.load(file)\n",
    "file.close()\n",
    "\n",
    "file = open(\"pickles/Logistic_Regression.pickle\", \"rb\") \n",
    "LogisticRegression_classifier = pickle.load(file)\n",
    "file.close()\n",
    "\n",
    "file = open(\"pickles/SGD_Classifier.pickle\", \"rb\") \n",
    "SGD_Classifier = pickle.load(file)\n",
    "file.close()\n",
    "\n",
    "file = open(\"pickles/Linear_SVC.pickle\", \"rb\") \n",
    "LinearSVC_classifier = pickle.load(file)\n",
    "file.close()\n",
    "\n",
    "file = open(\"pickles/Nu_SVC.pickle\", \"rb\") \n",
    "NuSVC_classifier = pickle.load(file)\n",
    "file.close()          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SZyj_69ViB7J"
   },
   "outputs": [],
   "source": [
    "voted_classifier = VoteClassifier(NB_classifier, MNB_classifier, BernoulliNB_classifier,\n",
    "                                  LogisticRegression_classifier, LinearSVC_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VKaD_4zJiB7O"
   },
   "source": [
    "## Module for party data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_M6lGfd6iB7P"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'MySQLdb'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-c9d17e766697>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtweepy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstreaming\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mStreamListener\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mMySQLdb\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'MySQLdb'"
     ]
    }
   ],
   "source": [
    "from tweepy import Stream\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy.streaming import StreamListener\n",
    "import os\n",
    "import MySQLdb\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "import time\n",
    "from unidecode import unidecode\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iF6TV3NMiB7U"
   },
   "outputs": [],
   "source": [
    "#consumer key, consumer secret, access token, access secret which is passed to the oauth for tweepy.\n",
    "consumer_key=\"Hg9dhbMGPQ3s2z5Jzde5oRwKu\"\n",
    "consumer_secret=\"iiJUk1zuWhUZdv4RICKuYxAqwVAVQ4MR3mX83RgPgYXcoFVUlF\"\n",
    "access_token=\"1068606021185282050-ZBPQSAo87KwUztDU8eLoCeTHS3h8zn\"\n",
    "access_secret=\"Yr7Sl41bmAZS9CRof4LZyHhUxLgUMflvVfepnC2FtpePq\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rP5XCaSiiB7a"
   },
   "outputs": [],
   "source": [
    "# create mySQL connection to the local host using MySQLdb module.\n",
    "# We are setting the charset to utf8mb4 to deal with smileys, emoticons, foriegn characters etc\n",
    "conn = MySQLdb.connect(\"localhost\",\"root\",\"mongo1234\",\"smdm\",use_unicode=True, charset=\"utf8mb4\")\n",
    "\n",
    "c = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RIWYs-qQiB7e"
   },
   "outputs": [],
   "source": [
    "# this is our SQL for adding the tweetID and results into our DB\n",
    "# always use %s no matter the type\n",
    "\n",
    "add_tweet = (\"INSERT INTO party_sentiment\"\n",
    "             \"(tweetID, party_name, dateTime, tweet,screen_name,followers_count,friends_count,\\\n",
    "             verified_bit, source,country, country_code, full_name, name, place_type,\\\n",
    "             reply_count, retweet_count, favorite_count, sentiment, confidence,num_sentiment)\"\n",
    "             \"VALUES ( %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hbWEg6TYiB7i"
   },
   "outputs": [],
   "source": [
    "def clean_tweet(tweet):\n",
    "    return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", tweet).split())\n",
    "\n",
    "# Twitter's results just give us back the tweet, but don't tell us which keyword it was found with\n",
    "# so, we have to use a keyword dictionary to search the tweet and match it back up to the party\n",
    "party_tags = dict()\n",
    "#BJP tags\n",
    "party_tags['modi'] = 'Bharatiya Janata Party'\n",
    "party_tags['namo'] = 'Bharatiya Janata Party'\n",
    "party_tags['namo'] = 'Bharatiya Janata Party'\n",
    "party_tags['phirekbaarmodisarkar'] = 'Bharatiya Janata Party'\n",
    "party_tags['bharatiyajanataparty']  = 'Bharatiya Janata Party'\n",
    "party_tags['bjp'] = 'Bharatiya Janata Party'\n",
    "party_tags['nationaldemocraticalliance'] = 'Bharatiya Janata Party'\n",
    "party_tags['nda'] = 'Bharatiya Janata Party'\n",
    "party_tags['vajpayee'] = 'Bharatiya Janata Party'\n",
    "#congress tags\n",
    "party_tags['indiannationalcongress'] = 'Indian National Congress'\n",
    "party_tags['incindia'] = 'Indian National Congress'\n",
    "party_tags['gandhi'] = 'Indian National Congress'\n",
    "party_tags['rahulgandhi'] = 'Indian National Congress'\n",
    "party_tags['soniagandhi'] = 'Indian National Congress'\n",
    "party_tags['sonia'] = 'Indian National Congress'\n",
    "party_tags['priyankagandhi'] = 'Indian National Congress'\n",
    "party_tags['rahulgandhiforpm'] = 'Indian National Congress'\n",
    "party_tags['inc'] = 'Indian National Congress'\n",
    "party_tags['congress'] = 'Indian National Congress'\n",
    "party_tags['amethi'] = 'Indian National Congress'\n",
    "\n",
    "\n",
    "\n",
    "#exclude punctuations\n",
    "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "\n",
    "# Open/create a file to append data to\n",
    "csvFile = open('crawl_result2.csv', 'a')\n",
    "\n",
    "#Use csv writer\n",
    "csvWriter = csv.writer(csvFile)\n",
    "\n",
    "#build the class used to process tweets to check for feeds\n",
    "class twitter_streaming(StreamListener):\n",
    "\n",
    "    def on_data(self, data):\n",
    "        all_data = json.loads(HTMLParser().unescape(data))\n",
    "        #https://developer.twitter.com/en/docs/tweets/data-dictionary/overview/tweet-object\n",
    "        #https://gist.github.com/hrp/900964\n",
    "        if 'text' in all_data:\n",
    "            #1\n",
    "            tweet = all_data['text']\n",
    "            tweet = unidecode(tweet)\n",
    "            #2\n",
    "            tweetID = all_data['id_str']\n",
    "            #3\n",
    "            source = all_data['source']\n",
    "            source = unidecode(source)\n",
    "            #4\n",
    "            if all_data['place']:\n",
    "                country = all_data['place']['country']\n",
    "                country = unidecode(country)\n",
    "                #5\n",
    "                country_code = all_data['place']['country_code']\n",
    "                country_code = unidecode(country_code)\n",
    "                #6\n",
    "                full_name = all_data['place']['full_name']\n",
    "                full_name = unidecode(full_name)\n",
    "                #7\n",
    "                name =  all_data['place']['name']\n",
    "                name = unidecode(name)\n",
    "                #8\n",
    "                place_type = all_data['place']['place_type']\n",
    "                place_type = unidecode(place_type)\n",
    "                #9\n",
    "            else:\n",
    "                country = country_code = full_name = name = place_type = \"0\"\n",
    "            \n",
    "            quote_count = all_data['quote_count']\n",
    "            #10\n",
    "            reply_count = all_data['reply_count']\n",
    "            #11\n",
    "            retweet_count = all_data['retweet_count']\n",
    "            #12\n",
    "            favorite_count = all_data['favorite_count']\n",
    "            #13\n",
    "            screen_name = all_data['user']['screen_name']\n",
    "            screen_name = unidecode(screen_name)\n",
    "            #13\n",
    "            followers_count = all_data['user']['followers_count']\n",
    "            #14\n",
    "            friends_count = all_data['user']['friends_count']\n",
    "            #15\n",
    "            verified = all_data['user']['verified']\n",
    "            #print(\"verified value is:\", verified)\n",
    "            #type(verified)\n",
    "        \n",
    "            \n",
    " \n",
    "            #tweetNoPunctuation = regex.sub('', tweet)\n",
    "            tweetNoPunctuation = clean_tweet(tweet)\n",
    "#we want to make sure while compiling tweets, we do not include the oens that are retweeted\n",
    "            if not all_data['retweeted'] and not tweet.startswith('RT') and 't.co' not in tweet:\n",
    "                sentiment_value, confidence = sentiment(tweetNoPunctuation)\n",
    "                #print(tweet, sentiment_value, confidence) #print output\n",
    "                \n",
    "                #value manipulations\n",
    "                if (sentiment_value.lower() == \"neg\"):\n",
    "                    num_sentiment=0\n",
    "                else:\n",
    "                    num_sentiment=1\n",
    "                    \n",
    "                if (verified == True):\n",
    "                    verified_bit = 1\n",
    "                    #print(\"Set\")\n",
    "                else:\n",
    "                    verified_bit = 0\n",
    "\n",
    "                \n",
    "                    \n",
    "                found = False\n",
    "                party = \"\"\n",
    "                for word in tweetNoPunctuation.split(\" \"):  \n",
    "                    if word.lower() in party_tags.keys():\n",
    "                        party_name = party_tags[word.lower()]\n",
    "                        print(\"Found keyword: \", word, \" belongs to party: \", party_name)\n",
    "                        found = True\n",
    "                        break\n",
    "\n",
    "                if found:\n",
    "                    created_at = time.strftime('%Y-%m-%d %H:%M:%S')  \n",
    "                    newID = (int)(all_data['id'])\n",
    "                    #twitter JSON is being parsed with queries below and using sentiment module, we are assigning confidence values\n",
    "                    # tweetID, party_name, dateTime, tweet, source,country, country_code, full_name, name, place_type,\\\n",
    "                    #  reply_count, retweet_count, favorite_count, result, confidence,num_sentiment\n",
    "                    tweet_data = (tweetID ,party_name, created_at, tweet,screen_name,followers_count,friends_count,\\\n",
    "                                  verified_bit, source,country, country_code,full_name,name, place_type,\\\n",
    "                                  reply_count, retweet_count, favorite_count, sentiment_value.lower(), confidence, num_sentiment)\n",
    "                    \n",
    "                    # Write a row to the CSV file. I use encode UTF-8\n",
    "                    csvWriter.writerow([tweetID ,party_name, created_at, tweet,screen_name,followers_count,friends_count,\\\n",
    "                                        verified_bit, source,country, country_code,full_name,name, place_type,\\\n",
    "                                  reply_count, retweet_count, favorite_count, sentiment_value.lower(), confidence, num_sentiment])\n",
    "                    \n",
    "                    c.execute(add_tweet, tweet_data)\n",
    "                    conn.commit()\n",
    "                    \n",
    "        #error handling, since tweepy tends to time out with twitter with out any reason closing the connection from their side\n",
    "        def on_limit(self, track):\n",
    "            print('Limit hit! Track = %s' % track)\n",
    "            return True\n",
    "\n",
    "        def on_error(self, status):\n",
    "            print(status)\n",
    "\n",
    "        def on_disconnect(self, notice):\n",
    "            print(notice)\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r4u33MI6iB7m"
   },
   "outputs": [],
   "source": [
    "# Twitter Authorization path\n",
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_secret)\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        # create twitter stream, which in turn will start streaming tweeets in JSON format, \n",
    "        #which we are using to query its metadata and store them separately on to the database\n",
    "        twitterStream = Stream(auth, twitter_streaming())\n",
    "\n",
    "        party keywords  to search for in tweets\n",
    "        twitterStream.filter(track=[\"modi\",\"namo\",\"narendramodi\",\"phirekbaarmodisarkar\", \"bharatiyajanataparty\",\"bjp\",\\\n",
    "                                     \"nationaldemocraticalliance\", \"nda\",\\\n",
    "                                     \"vajpayee\", \"indiannationalcongress\", \"gandhi\", \"rahulgandhi\", \"soniagandhi\",\"priyankagandhi\",\\\n",
    "                                     \"rahulgandhiforpm\"], languages=[\"en\"])\n",
    "        \n",
    "       # twitterStream.filter(track=[\"amethi\", \"incindia\", \"indiannationalcongress\",\"sonia\", \"gandhi\", \"rahulgandhi\", \"soniagandhi\",\"priyankagandhi\",\\\n",
    "                                    \"rahulgandhiforpm\"], languages=[\"en\"])\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "cursor.close()\n",
    "conn.close()\n",
    "csvFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "amMBOxBmiB7v"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a5TQ4u1JiB70"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BKESNnoAiB74"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZPckXfKbiB77"
   },
   "source": [
    "https://www.dataquest.io/blog/matplotlib-tutorial/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "midSkd5aiB79"
   },
   "source": [
    "https://ritetag.com/best-hashtags-for/bjp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A7VAwMAuiB7-"
   },
   "source": [
    "### Another"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "heate8SKiB7_"
   },
   "outputs": [],
   "source": [
    "out1 = \"twitter-feed.txt\"\n",
    "os.remove(out1) if os.path.exists(out1) else None\n",
    "\n",
    "out2 = \"twitter-feed-textblob.txt\"\n",
    "os.remove(out2) if os.path.exists(out2) else None\n",
    "\n",
    "positive, negative, neutral = 0.0, 0.0, 0.0\n",
    "\n",
    "    \n",
    "class listener(StreamListener):    \n",
    "    def on_data(self, data):\n",
    "\n",
    "\n",
    "        all_data = json.loads(data)\n",
    "        tweet = clean_tweet(all_data[\"text\"])\n",
    "        print (tweet)\n",
    "        \n",
    "        category, confidence = sentiment(tweet)  # Custom-sentiment\n",
    "        print (\"Custom Sentiment : \", category, \"-->\", confidence)\n",
    "\n",
    "\n",
    "        if confidence * 100 >= 80:\n",
    "            output = open(out1, \"a\")\n",
    "            output.write(category)\n",
    "            output.write(\"\\n\")\n",
    "            output.close()\n",
    "        \n",
    "        blob = TextBlob(tweet)\n",
    "        confidence = blob.sentiment.polarity  # Textblob-sentiment\n",
    "        if confidence > 0:\n",
    "            category = \"pos\"\n",
    "#             positive += 1.0\n",
    "        else:\n",
    "            category = \"neg\" \n",
    "#             negative += 1.0\n",
    "\n",
    "        \n",
    "        print (\"TextBlob Sentiment : \", category, \"-->\", \"{:0.2f}\" .format(confidence))\n",
    "        print ()\n",
    "        \n",
    "        output = open(out2, \"a\")\n",
    "        output.write(category)\n",
    "        output.write(\"\\n\")\n",
    "        output.close()\n",
    "            \n",
    "        return True\n",
    "    \n",
    "    def on_error(self, status):\n",
    "        print (status)     \n",
    "        \n",
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_secret)\n",
    "\n",
    "query = input(\"Enter your keyword to be searched in Twitter: \")\n",
    "\n",
    "output = open(out1, \"a\")\n",
    "output.write(query)\n",
    "output.write(\"\\n\")\n",
    "output.close()\n",
    "\n",
    "output = open(out2, \"a\")\n",
    "output.write(query)\n",
    "output.write(\"\\n\")\n",
    "output.close()\n",
    "\n",
    "twitterStream = Stream(auth, listener())\n",
    "twitterStream.filter(track=[query])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5l3CfTcniB8G"
   },
   "outputs": [],
   "source": [
    "import html\n",
    "import os\n",
    "import plotly\n",
    "import socket\n",
    "\n",
    "from twython import Twython\n",
    "from twython import TwythonAuthError, TwythonError, TwythonRateLimitError\n",
    "\n",
    "def chart(positive, negative):\n",
    "    \"\"\"Return a pie chart for specified sentiments as HTML.\"\"\"\n",
    "\n",
    "    # offline plot\n",
    "    # https://plot.ly/python/pie-charts/\n",
    "    # https://plot.ly/python/reference/#pie\n",
    "    figure = {\n",
    "        \"data\": [\n",
    "            {\n",
    "                \"labels\": [\"positive\", \"negative\"],\n",
    "                \"hoverinfo\": \"none\",\n",
    "                \"marker\": {\n",
    "                    \"colors\": [\n",
    "                        \"rgb(0,255,00)\",\n",
    "                        \"rgb(255,0,0)\"\n",
    "                    ]\n",
    "                },\n",
    "                \"type\": \"pie\",\n",
    "                \"values\": [positive, negative]\n",
    "            }\n",
    "        ],\n",
    "        \"layout\": {\n",
    "            \"showlegend\": True\n",
    "            }\n",
    "    }\n",
    "    return plotly.offline.plot(figure, output_type=\"div\", show_link=False, link_text=False)\n",
    "# generate chart\n",
    "chart = chart(positive, negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5k77wPcbiB8N"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E-g41KQtiB8T"
   },
   "source": [
    "### SQL Queries\n",
    "\n",
    "use smdm;\n",
    "#Table, Create Table\n",
    " CREATE TABLE `party_sentiment` (\n",
    " `id_t1` bigint NOT NULL AUTO_INCREMENT,\n",
    "  `tweetID` varchar(200) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci NOT NULL,\n",
    "  `party_name` varchar(200) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci NOT NULL,\n",
    "  `dateTime` datetime(6) DEFAULT NULL,\n",
    "  `tweet` varchar(200) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci NOT NULL,\n",
    "  `source` varchar(200) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci NOT NULL,\n",
    "  `country` varchar(200) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci,\n",
    "  `country_code` varchar(200) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci,\n",
    "  `full_name` varchar(200) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci,\n",
    "  `name` varchar(200) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci,\n",
    "  `place_type` varchar(200) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci,\n",
    "  `quote_count` int(11) DEFAULT NULL,\n",
    "  `reply_count` int(11) DEFAULT NULL,\n",
    "  `retweet_count` int(11) DEFAULT NULL,\n",
    "  `favorite_count` int(11) DEFAULT NULL,\n",
    "  `result` varchar(6) DEFAULT NULL,\n",
    "  `confidence` float DEFAULT NULL,\n",
    "  `num_sentiment` int(11) DEFAULT NULL,\n",
    "  primary key (id_t1)\n",
    ")ENGINE=InnoDB DEFAULT CHARSET=utf8;\n",
    "\n",
    "#delete all data from table\n",
    "DELETE FROM party_sentiment;\n",
    "\n",
    "#seed auto increment\n",
    "ALTER TABLE party_sentiment AUTO_INCREMENT = 1;\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nduyJe9KiB8V"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "2. Sentiment Analysis -v2.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "dev1",
   "language": "python",
   "name": "dev1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
